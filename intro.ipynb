{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intro.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP1JSh/sUPXVl0GuCj2eGKX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/okanbuyuktepe/Deep-learning-Exercises/blob/master/intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHD3ywUHGIl4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install theano"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGBNU0naGL67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example of Theano Library\n",
        "import theano\n",
        "from theano import tensor\n",
        "# declare two symbolic floating-point scalars\n",
        "a = tensor.dscalar()\n",
        "b = tensor.dscalar()\n",
        "# create a simple symbolic expression\n",
        "c = a + b\n",
        "# convert the expression into a callable object that takes (a,b) and computes c\n",
        "f = theano.function([a,b], c)\n",
        "# bind 1.5 to 'a', 2.5 to 'b', and evaluate 'c'\n",
        "result = f(1.5,2.5)\n",
        "print(result)\n",
        "# Keras is a wrapper library that hides Theano completely and provides a very\n",
        "# simple API to work with to create deep learning models."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u7Cg5gQIpF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TensorFlow is an open source library for fast numerical computing. It was created\n",
        "# and is maintained by Google ans released under Apache 2.0 open source licence.\n",
        "'''\n",
        "-> Nodes: Nodes perform computation and have zero or more inputs and outputs. Data that\n",
        "moves between nodes are known as tensors, which are multi-dimensional arrays of real\n",
        "values.\n",
        "-> Edges: The graph defines the flow of data, branching, looping and updates to state.\n",
        "Special edges can be used to synchronize behavior within the graph, for example waiting\n",
        "for computation on a number of inputs to complete.\n",
        "-> Operation: An operation is a named abstract computation which can take input attributes\n",
        "and produce output attributes. For example, you could define an add or multiply operation.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P7nHABVKmbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example of TensorFlow library\n",
        "import tensorflow as tf\n",
        "# declare two symbolic floating-point scalars\n",
        "a = tf.compat.v1.placeholder(tf.float32)\n",
        "b = tf.compat.v1.placeholder(tf.float32)\n",
        "# create a simple symbolic expression using the add function\n",
        "add = tf.add(a,b)\n",
        "# bind 1.5 to 'a', 2.5 to 'b', and evaluate 'c'\n",
        "sess = tf.compat.v1.Session()\n",
        "binding = {a:1.5, b:2.5}\n",
        "c = sess.run(add, feed_dict = binding)\n",
        "print(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyFswRYMMu-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Two of the top numerical platforms in Python that provide the basis for deep learning research\n",
        "# and development are Theano and TensorFlow. Both are very powerful libraries, but both can\n",
        "# be difficult to use directly for creating deep learning models.\n",
        "# Keras Python library that provides a clean and convenient way to create a range of deep\n",
        "# learning models on top of Theano or TensorFlow."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG6LL3adM7FE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend\n",
        "print(backend.backend())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7Ebnem3MPlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Keras\n",
        "'''\n",
        "-Define your model : Create a Sequential model and add configured layers\n",
        "-Compile your model : Specify loss function and optimizer and call the compile()\n",
        "-Fit your model : Train the model on a sample of data by calling the fit()\n",
        "-Make predictions : Use the model to generate predictions on new data by calling\n",
        "functions such as evaluate() or predict()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLRlzh1wTkh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Multi Layer Perceptron\n",
        "'''\n",
        "The power of neural networks come from their ability to learn the representation in your\n",
        "training data and how to best relate it to the output variable that you want to predict. In\n",
        "this sense neural networks learn a mapping.\n",
        "\n",
        "The weighted inputs are summed and passed through an activation function, sometimes called a\n",
        "transfer function. An activation function is a simple mapping of summed weighted input to the\n",
        "output of the neuron. It is called an activation function because it governs the threshold at\n",
        "which the neuron is activated and the strength of the output signal.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EW7A9NzVfjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stochastic Gradient Descent\n",
        "'''\n",
        "The classical and still preferred training algorithm for neural networks is called stochastic\n",
        "gradient descent.\n",
        "\n",
        "The output of the network is compared to the expected output and an error is calculated.\n",
        "This error is then propagated back through the network, one layer at a time, and the weights\n",
        "are updated according to the amount that they contributed to the error. This clever bit of math\n",
        "is called the Backpropagation algorithm.\n",
        "\n",
        "One round of updating the network for the entire training dataset is called an\n",
        "epoch. \n",
        "\n",
        "'''\n",
        "# Weight Updates\n",
        "'''\n",
        "The weights in the network can be updated from the errors calculated for each training example\n",
        "and this is called online learning. \n",
        "\n",
        "Alternatively, the errors can be saved up across all of the training examples and the network\n",
        "can be updated at the end. This is called batch learning and is often more stable.\n",
        "\n",
        "The amount that weights are updated is controlled by a configuration parameter called the learning rate.\n",
        "\n",
        "The update equation can be complemented with additional configuration terms that you can set:\n",
        "\n",
        "-> Momentum : is a term that incorporates the properties from the previous weight update\n",
        "to allow the weights to continue to change in the same direction even when there is less\n",
        "error being calculated.\n",
        "\n",
        "-> Learning Rate Decay : is used to decrease the learning rate over epochs to allow the\n",
        "network to make large changes to the weights at the beginning and smaller fine tuning\n",
        "changes later in the training schedule."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
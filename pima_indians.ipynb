{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pima_indians.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP/coym6mt7UAPfAtS5BnPD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/okanbuyuktepe/Deep-learning-Exercises/blob/master/pima_indians.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is9t-1ScZd1h",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Pima Indians Onset of Diabetes**\n",
        "- It is a binary classification problem (onset of diabetes as 1 or not as 0).\n",
        "- Below lists the eight attributes for the dataset:\n",
        "  - Number of times pregnant.\n",
        "  - Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\n",
        "  - Diastolic blood pressure (mm Hg).\n",
        "  - Triceps skin fold thickness (mm).\n",
        "  - 2-Hour serum insulin (mu U/ml).\n",
        "  - Body mass index.\n",
        "  - Diabetes pedigree function.\n",
        "  - Age (years).\n",
        "  - Class, onset of diabetes within five years."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozL0vngYZWXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Whenever we work with machine learning algorithms that use a stochastic process (e.g. random\n",
        "numbers), it is a good idea to initialize the random number generator with a fixed seed value.\n",
        "This is so that you can run the same code again and again and get the same result.\n",
        "'''\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "# fix random seed for reproducibility\n",
        "np.random.seed(7)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skE8sCkPcJgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data\n",
        "url = ('https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv')\n",
        "dataset = np.loadtxt(url, delimiter=',')\n",
        "# split into input and output variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8DsIlfBcppA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Model\n",
        "# The first thing to get right is to ensure the input layer has the right number of inputs.\n",
        "# This can be specified when creating the first layer with the input_dim argument and setting it to 8 for the 8 input variables.\n",
        "# There are heuristics that we can use and often the best network structure is found through a process of trial and error \n",
        "# experimentation.\n",
        "# Generally, you need a network large enough to capture the structure of the problem if that helps at all. In this example we will use a\n",
        "# fully-connected network structure with three layers.\n",
        "# Fully connected layers are defined using the Dense class."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-ZwsTTzctSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12,input_dim=8,activation='relu'))\n",
        "model.add(Dense(8,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhJP76TWh9RK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile Model\n",
        "# Compiling the model uses the efficient numerical libraries under the covers (the so-called backend) such as Theano or TensorFlow.\n",
        "# Remember training a network means finding the best set of weights to make predictions for this problem.\n",
        "# We must specify the loss function to use to evaluate a set of weights, the optimizer used to search through different weights\n",
        "# for the network and any optional metrics we would like to collect and report during training.\n",
        "# In this case we will use logarithmic loss, which for a binary classiffication problem is defined in Keras as binary_crossentropy. \n",
        "# Will also use the efficient gradient descent algorithm adam for no other reason that it is an efficient default.\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0u170H0jvHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit the model\n",
        "# We can also set the number of instances that are evaluated before a weight update in the network is performed called the\n",
        "# batch size and set using the batch_size argument.\n",
        "model.fit(X, Y, epochs=150, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsJOatTiksvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate Model\n",
        "# We can evaluate the performance of the network on the same dataset. This will only give us an idea of how well we have modeled\n",
        "# the dataset (e.g. train accuracy), but no idea of how well the algorithm might perform on new data.\n",
        "\n",
        "scores = model.evaluate(X, Y)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2Hv5HTuOS-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use a Automatic Verification Dataset\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy\n",
        "\n",
        "numpy.random.seed(7)\n",
        "url = ('https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv')\n",
        "dataset = numpy.loadtxt(url, delimiter=',')\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12,input_dim=8, activation='relu'))\n",
        "model.add(Dense(8,activation='relu'))\n",
        "model.add(Dense(1,activation = 'sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
        "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvBP0U-EQdXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use a Manual Verification Dataset\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy\n",
        "\n",
        "numpy.random.seed(7)\n",
        "url = ('https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv')\n",
        "dataset = numpy.loadtxt(url, delimiter=',')\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size =0.33, random_state=7)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "model.add(Dense(8,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=150, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkqaKSgfUrhZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Manual k-fold Cross-Validation\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy\n",
        "\n",
        "numpy.random.seed(7)\n",
        "url = ('https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv')\n",
        "dataset = numpy.loadtxt(url, delimiter=',')\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# define 10-fold cross validation test harness\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle= True, random_state=7)\n",
        "cvscores = []\n",
        "for train, test in kfold.split(X, Y):\n",
        "  model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "  model.add(Dense(8, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  model.fit(X[train], Y[train], epochs=150, batch_size=10, verbose=0)\n",
        "\n",
        "  #evaluate the model\n",
        "  scores = model.evaluate(X[test], Y[test], verbose=0)\n",
        "  print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "  cvscores.append(scores[1]*100)\n",
        "\n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R6txIana583",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate a Neural Network using scikit-learn\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy\n",
        "\n",
        "numpy.random.seed(7)\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "  model.add(Dense(8, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "dataset = numpy.loadtxt(url, delimiter=',')\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, epochs=150, batch_size = 10, verbose=0)\n",
        "kfold= StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onGochDsjbGt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "0298c942-5295-4ca8-9a0b-c68be91b2bf9"
      },
      "source": [
        "# MLP for Pima Indians Dataset with grid search via sklearn\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(12, input_dim=8, kernel_initializer=init, activation='relu'))\n",
        "  model.add(Dense(8, kernel_initializer=init, activation='relu'))\n",
        "  model.add(Dense(1,kernel_initializer=init, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer = optimizer, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "url = ('https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv')\n",
        "dataset = numpy.loadtxt(url, delimiter=',')\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "# grid search epochs, batch size and optimizer\n",
        "optimizers = ['rmsprop', 'adam']\n",
        "inits = ['glorot_uniform', 'normal', 'uniform']\n",
        "epochs = [50, 100, 150]\n",
        "batches = [5, 10, 20]\n",
        "param_grid = dict(optimizer = optimizers, epochs=epochs, batch_size=batches, init=inits)\n",
        "grid = GridSearchCV(estimator = model, param_grid=param_grid)\n",
        "grid_result = grid.fit(X, Y)\n",
        "# summarize results\n",
        "print('Best: %f using %s' % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print('%f (%f) with: %r' % (mean, stdev, param))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.760513 using {'batch_size': 5, 'epochs': 150, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.696690 (0.039341) with: {'batch_size': 5, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.724056 (0.052683) with: {'batch_size': 5, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.712240 (0.027752) with: {'batch_size': 5, 'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.710975 (0.051658) with: {'batch_size': 5, 'epochs': 50, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.704499 (0.030901) with: {'batch_size': 5, 'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.679696 (0.028185) with: {'batch_size': 5, 'epochs': 50, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.688787 (0.015359) with: {'batch_size': 5, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.746202 (0.040712) with: {'batch_size': 5, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.739615 (0.035654) with: {'batch_size': 5, 'epochs': 100, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.748799 (0.045232) with: {'batch_size': 5, 'epochs': 100, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.731772 (0.013237) with: {'batch_size': 5, 'epochs': 100, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.747483 (0.042121) with: {'batch_size': 5, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.725363 (0.043215) with: {'batch_size': 5, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.742271 (0.034506) with: {'batch_size': 5, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.746159 (0.035726) with: {'batch_size': 5, 'epochs': 150, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.753994 (0.036116) with: {'batch_size': 5, 'epochs': 150, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.759138 (0.036871) with: {'batch_size': 5, 'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.760513 (0.044065) with: {'batch_size': 5, 'epochs': 150, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.648655 (0.104334) with: {'batch_size': 10, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.683669 (0.033630) with: {'batch_size': 10, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.709736 (0.049956) with: {'batch_size': 10, 'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.704439 (0.019388) with: {'batch_size': 10, 'epochs': 50, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.703251 (0.057962) with: {'batch_size': 10, 'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.698031 (0.044919) with: {'batch_size': 10, 'epochs': 50, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.704541 (0.041767) with: {'batch_size': 10, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.691452 (0.040022) with: {'batch_size': 10, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.734377 (0.045866) with: {'batch_size': 10, 'epochs': 100, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.746125 (0.031619) with: {'batch_size': 10, 'epochs': 100, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.726585 (0.020290) with: {'batch_size': 10, 'epochs': 100, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.726628 (0.047252) with: {'batch_size': 10, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.705789 (0.064129) with: {'batch_size': 10, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.701808 (0.048063) with: {'batch_size': 10, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.746151 (0.034769) with: {'batch_size': 10, 'epochs': 150, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.731856 (0.041688) with: {'batch_size': 10, 'epochs': 150, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.737000 (0.034909) with: {'batch_size': 10, 'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.734394 (0.044935) with: {'batch_size': 10, 'epochs': 150, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.683677 (0.044325) with: {'batch_size': 20, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.696622 (0.027783) with: {'batch_size': 20, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.703175 (0.024965) with: {'batch_size': 20, 'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.704533 (0.044583) with: {'batch_size': 20, 'epochs': 50, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.703200 (0.028815) with: {'batch_size': 20, 'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.712325 (0.041361) with: {'batch_size': 20, 'epochs': 50, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.692810 (0.053441) with: {'batch_size': 20, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.716230 (0.042444) with: {'batch_size': 20, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.735795 (0.041412) with: {'batch_size': 20, 'epochs': 100, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.718776 (0.043406) with: {'batch_size': 20, 'epochs': 100, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.731805 (0.031220) with: {'batch_size': 20, 'epochs': 100, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.747500 (0.037377) with: {'batch_size': 20, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.724056 (0.044653) with: {'batch_size': 20, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.688846 (0.024233) with: {'batch_size': 20, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.740956 (0.036695) with: {'batch_size': 20, 'epochs': 150, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.747432 (0.034263) with: {'batch_size': 20, 'epochs': 150, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.750072 (0.038813) with: {'batch_size': 20, 'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.733096 (0.016645) with: {'batch_size': 20, 'epochs': 150, 'init': 'uniform', 'optimizer': 'adam'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSYTZvwur_44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}